{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wi2aH1D5gTp7",
        "outputId": "0ef0e9f2-cb1a-4070-fd70-a62d4e856d83"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting uts\n",
            "  Downloading uts-0.0.4.tar.gz (5.4 kB)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.7/dist-packages (from uts) (1.19.5)\n",
            "Building wheels for collected packages: uts\n",
            "  Building wheel for uts (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for uts: filename=uts-0.0.4-py3-none-any.whl size=6678 sha256=3684d603f57909639524e13e045c0a914238d7dca64e3d019c9b6731eda94454\n",
            "  Stored in directory: /root/.cache/pip/wheels/41/71/ba/61ac322199ea88f9cd28b710ac1bef6c821e1ac71a635104fc\n",
            "Successfully built uts\n",
            "Installing collected packages: uts\n",
            "Successfully installed uts-0.0.4\n"
          ]
        }
      ],
      "source": [
        "%pip install uts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "2rsQV8Fxpz1V"
      },
      "outputs": [],
      "source": [
        "import uts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0g11Tx1JqMut"
      },
      "outputs": [],
      "source": [
        "mylines = []                             # Declare an empty list named mylines.\n",
        "with open ('script.txt', 'rt') as myfile: # Open script.txt for reading text data.\n",
        "    for myline in myfile:                # For each line, stored as myline,\n",
        "        mylines.append(myline)           # add its contents to mylines.\n",
        "mydialogue = []\n",
        "for i in range(len(mylines)):\n",
        "    if len(mylines[i])- len(mylines[i].lstrip())>9:\n",
        "        mydialogue.append(mylines[i])\n",
        "\n",
        "mydial = ''.join(mydialogue)\n",
        "dial = mydial.split('                     ')\n",
        "dialogue = [s.replace(\"\\n\", \"\") for s in dial]\n",
        "dialoguetot = []\n",
        "for i in range (len(dialogue)):\n",
        "    dialogue_i = []\n",
        "    dialogue_i.append(dialogue[i])\n",
        "    dialoguetot.append(dialogue_i)\n",
        "\n",
        "# this is to take out all empty strings from the dialogue \n",
        "realdialogue = [] \n",
        "for i in range (len(dialoguetot)):\n",
        "  if any(c.isalpha() for c in dialoguetot[i][0]) == True:\n",
        "    realdialogue.append(dialoguetot[i])\n",
        "  else : \n",
        "    pass\n",
        "\n",
        "# this is to block our sentences into group of conversations\n",
        "conv = [realdialogue[0]]\n",
        "conv1 = []\n",
        "for i in range (1, len(realdialogue)):\n",
        "  if i % 10 == 0:\n",
        "    conv1.append(realdialogue[i])\n",
        "    conv.append(conv1)\n",
        "    conv1 = []\n",
        "  else :\n",
        "    conv1.append(realdialogue[i])\n",
        "\n",
        "# And this to make list of lists \n",
        "realconv =[]\n",
        "for i in range(0, len(conv)):\n",
        "  flat_conv = []\n",
        "  for sublist in conv[i]:\n",
        "    for item in sublist:\n",
        "        flat_conv.append(item)\n",
        "  realconv.append(flat_conv)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# This cell is for the test where I divide script in 3 parts \n",
        "import math\n",
        "n = len(realdialogue)\n",
        "conv1 = []\n",
        "conv2 = []\n",
        "conv3 = []\n",
        "for i in range(0,math.ceil(n/3)):\n",
        "  conv1.append(realdialogue[i])\n",
        "for i in range(math.ceil(n/3), math.ceil((2*n)/3)):\n",
        "  conv2.append(realdialogue[i])\n",
        "for i in range(math.ceil((2*n)/3),n):\n",
        "  conv3.append(realdialogue[i])\n",
        "conv = [conv1,conv2,conv3]\n",
        "\n",
        "#to make it list of lists it does not change\n",
        "realconv =[]\n",
        "for i in range(0, len(conv)):\n",
        "  flat_conv = []\n",
        "  for sublist in conv[i]:\n",
        "    for item in sublist:\n",
        "        flat_conv.append(item)\n",
        "  realconv.append(flat_conv)"
      ],
      "metadata": {
        "id": "4-h-IYTbUXX1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# this cell is for when I use the data set of tripod where I can get the segmented scripts so I divide my dialogue into scenes \n",
        "# See sentence embeddings to generate segmented script \n",
        "import math \n",
        "mylines = []                             # Declare an empty list named mylines.\n",
        "with open ('segmentscript.txt', 'rt') as myfile: # Open script.txt for reading text data.\n",
        "    for myline in myfile:                # For each line, stored as myline,\n",
        "        mylines.append(myline)           # add its contents to mylines.\n",
        "mydialogue = []\n",
        "dialoguetest = []\n",
        "for i in range(len(mylines)):\n",
        "    if mylines[i][0] == '=':\n",
        "      mydialogue.append(dialoguetest)\n",
        "      dialoguetest =[]\n",
        "    else :\n",
        "      dialoguetest.append(mylines[i])\n",
        "\n",
        "dialogue = []\n",
        "for i in range(len(mydialogue)):\n",
        "  test1 = []\n",
        "  for j in range(len(mydialogue[i])):\n",
        "    if len(mydialogue[i][j])- len(mydialogue[i][j].lstrip())>9:\n",
        "        test1.append(mydialogue[i][j])\n",
        "  dialogue.append(test1)\n",
        "\n",
        "dialoguetot = []\n",
        "for i in range(len(dialogue)):\n",
        "    mydial = ''.join(dialogue[i])                    # In this part what i do is that I put in the good form for the future preprocessing operations\n",
        "    dial = mydial.split('                 ')    # I take out all the blank spaces and \\n and I do a list of list and each list is a sentence said by a new character\n",
        "    dialogue1 = [s.replace(\"\\n\", \"\") for s in dial]\n",
        "    dialoguetot.append(dialogue1)\n",
        "\n",
        "realdial = []\n",
        "for i in range(len(dialoguetot)):\n",
        "  if len(dialoguetot[i]) >1 :\n",
        "    realdial.append(dialoguetot[i])\n",
        "\n",
        "realdialogue1 = []  #made to take out of dialogue all empty strings\n",
        "for i in range (len(realdial)):\n",
        "  conv =[]\n",
        "  for j in range(len(realdial[i])):\n",
        "    if j >= 1 :\n",
        "      conv.append(realdial[i][j])\n",
        "  realdialogue1.append(conv)"
      ],
      "metadata": {
        "id": "xE-au3vvGV2n"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "Og9scdBXpz1Y"
      },
      "outputs": [],
      "source": [
        "import gensim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mweLiI56ly2v"
      },
      "outputs": [],
      "source": [
        "# I modify the remove name function from the original one to adapt to my script and what I do \n",
        "# Is that I use the fact that each dialogue starts with the name then many spaces so I take this part out\n",
        "# this is for the case where I have one big list \n",
        "def remove_name2(conversation):\n",
        "  paroles = []\n",
        "  for i in range(0, len(conversation)):\n",
        "    newdialogue = conversation[i][0].split('   ')\n",
        "    for i in range (0,len(newdialogue)):\n",
        "      if newdialogue[i].isupper() == False :\n",
        "        paroles.append(newdialogue[i])\n",
        "  return paroles\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# remove name test when I divide into list of lists whether divide into blovks of 10 utterances or scene segmentation\n",
        "def remove_name3(conversation):\n",
        "  paroles = []\n",
        "  for i in range(0, len(conversation)):\n",
        "    newdialogue = conversation[i].split('       ')\n",
        "    for i in range (0,len(newdialogue)):\n",
        "      if newdialogue[i].isupper() == False :\n",
        "        paroles.append(newdialogue[i])\n",
        "  return paroles"
      ],
      "metadata": {
        "id": "2vxACYkREcN6"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B7jT2adIpz1Y"
      },
      "outputs": [],
      "source": [
        "# Original remove name function that is now unused\n",
        "def remove_name(s):\n",
        "    temp = \"\"\n",
        "    flag = 0\n",
        "    for i in range(0, len(s)):\n",
        "        if s[i] == ':':\n",
        "            flag = 1\n",
        "            continue\n",
        "        if flag == 1:\n",
        "            temp += s[i]\n",
        "    return temp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 392
        },
        "id": "28ilztT7pz1Z",
        "outputId": "2c5d117e-d5a1-47c6-b33d-712a67355eb5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-1ac6bbe9be0d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mtable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaketrans\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstring\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpunctuation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0msentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mphrases\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mremove_name3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrealconv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mphrases\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m       \u001b[0mtemp_sent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mphrases\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-9-31384e2aecab>\u001b[0m in \u001b[0;36mremove_name3\u001b[0;34m(conversation)\u001b[0m\n\u001b[1;32m      3\u001b[0m   \u001b[0mparoles\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconversation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mnewdialogue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconversation\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'       '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnewdialogue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mnewdialogue\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misupper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;32mFalse\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'split'"
          ]
        }
      ],
      "source": [
        "# I modified this cell a little from original because my remove name function directly returns a list with all sentences said \n",
        "# this one is for the case where we have one list with all sentences\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "import string\n",
        "from nltk.tokenize import word_tokenize\n",
        "table = str.maketrans('', '', string.punctuation)\n",
        "sentences = []\n",
        "phrases = remove_name2(realdialogue)\n",
        "for i in range(0, len(phrases)):\n",
        "      temp_sent = phrases[i]\n",
        "      tokens = word_tokenize(temp_sent)\n",
        "      tokens = [w.lower() for w in tokens]\n",
        "      stripped = [w.translate(table) for w in tokens]\n",
        "      sentences.append(stripped)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# This is for the case where we have a list of lists of sentences, when dialogues divided into scenes\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "import string\n",
        "from nltk.tokenize import word_tokenize\n",
        "table = str.maketrans('', '', string.punctuation)\n",
        "sentences = []\n",
        "for i in range (0, len(realdialogue1)):\n",
        "      phrases = remove_name3(realdialogue1[i])\n",
        "      for j in range(0,len(phrases)):\n",
        "        temp_sent = phrases[j]\n",
        "        tokens = word_tokenize(temp_sent)\n",
        "        tokens = [w.lower() for w in tokens]\n",
        "        stripped = [w.translate(table) for w in tokens]\n",
        "        sentences.append(stripped)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Ob22fuZFESQ",
        "outputId": "bbca24c5-33e7-43ee-c49d-39451e89f03a"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "XdGyfK56gvC6"
      },
      "outputs": [],
      "source": [
        "%load C99.py "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "oMSEHXnmpz1Z"
      },
      "outputs": [],
      "source": [
        " from C99 import C99"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "nVmTlM6Bpz1a"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "with open('diehard_sentence_transformer.pkl', 'rb') as f:\n",
        "   diehard = pickle.load(f)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xkRuv_-8pz1a"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "KaNc_xf0pz1a"
      },
      "outputs": [],
      "source": [
        "def encode_convs(profix):\n",
        "    model = C99(window = 4, std_coeff = 1)\n",
        "    sent_label = []\n",
        "    with open(profix+\"_sentence_transformer.pkl\", 'rb') as f:\n",
        "        data = pickle.load(f)\n",
        "    for i in range(0, len(data)):\n",
        "        boundary = model.segment(data[i])\n",
        "        temp_labels = []\n",
        "        l = 0\n",
        "        for j in range(0, len(boundary)):\n",
        "            if boundary[j] == 1:\n",
        "                l += 1\n",
        "            temp_labels.append(l)\n",
        "        sent_label.append(temp_labels)\n",
        "    \n",
        "    with open(profix + '_sent_c99_label.pkl', 'wb') as f:\n",
        "        pickle.dump(sent_label, f)\n",
        "    \n",
        "    return sent_label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "A7vod42Upz1b"
      },
      "outputs": [],
      "source": [
        "l = encode_convs('diehard')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vpe1pqqYpz1b"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HT1av52Jpz1b"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HEap4zgvpz1c"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Topic_Segment.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}